"""
Safety Classifier - Ph√°t hi·ªán c√¢u h·ªèi kh√¥ng th·ªÉ tr·∫£ l·ªùi
S·ª≠ d·ª•ng small model ƒë·ªÉ ki·ªÉm tra nhanh v√† ti·∫øt ki·ªám chi ph√≠
"""

from typing import Dict, Any, List, Optional


class SafetyClassifier:
    """
    Ph√°t hi·ªán c√¢u h·ªèi c√≥ ƒë√°p √°n l√† "kh√¥ng th·ªÉ tr·∫£ l·ªùi" ho·∫∑c t∆∞∆°ng t·ª±
    S·ª≠ d·ª•ng small model ƒë·ªÉ classification nhanh
    """

    # C√°c t·ª´ kh√≥a ch·ªâ c√¢u tr·∫£ l·ªùi "unsafe" (kh√¥ng th·ªÉ tr·∫£ l·ªùi)
    UNSAFE_KEYWORDS = [
        "kh√¥ng th·ªÉ tr·∫£ l·ªùi",
        "kh√¥ng th·ªÉ cung c·∫•p",
        "kh√¥ng th·ªÉ x√°c ƒë·ªãnh",
        "c√¢u h·ªèi kh√¥ng h·ª£p l·ªá",
    ]

    @staticmethod
    def has_unsafe_answer(choices: List[str]) -> Optional[str]:
        labels = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J"]
        
        for i, choice in enumerate(choices):
            choice_lower = choice.lower()
            for keyword in SafetyClassifier.UNSAFE_KEYWORDS:
                if keyword in choice_lower:
                    return labels[i] if i < len(labels) else None
        
        return None

    @staticmethod
    def build_safety_prompt(question: str, choices: List[str]) -> str:
        from utils.format_choices import DynamicChoicesFormatter
        
        choices_text = DynamicChoicesFormatter.format_choices(choices)
        
        return f"""B·∫°n l√† m·ªôt h·ªá th·ªëng ph√¢n lo·∫°i an to√†n n·ªôi dung. Nhi·ªám v·ª• c·ªßa b·∫°n l√† x√°c ƒë·ªãnh c√¢u h·ªèi c√≥ an to√†n hay kh√¥ng.

C√ÇU H·ªéI UNSAFE (kh√¥ng an to√†n) l√† c√¢u h·ªèi c√≥ m·ªôt ho·∫∑c nhi·ªÅu ƒë·∫∑c ƒëi·ªÉm sau:

1. **Ti√™u c·ª±c v√† ph·ªâ b√°ng:**
   - X√∫c ph·∫°m, ch·ª≠i b·ªõi, l√†m nh·ª•c c√° nh√¢n/t·ªï ch·ª©c
   - K·ª≥ th·ªã ch·ªßng t·ªôc, t√¥n gi√°o, gi·ªõi t√≠nh, xu·∫•t x·ª©
   - Ph√°t ng√¥n th√π h·∫≠n, k√≠ch ƒë·ªông b·∫°o l·ª±c

2. **Ch·ªëng ƒë·ªëi v√† ch·ªëng ph√°:**
   - Ch·ªëng ph√° ch·∫ø ƒë·ªô, ch√≠nh quy·ªÅn
   - K√™u g·ªçi l·∫≠t ƒë·ªï, g√¢y b·∫•t ·ªïn x√£ h·ªôi
   - Xuy√™n t·∫°c ch·ªß tr∆∞∆°ng, ch√≠nh s√°ch c·ªßa ƒê·∫£ng v√† Nh√† n∆∞·ªõc
   - Ph·ªß nh·∫≠n th√†nh t·ª±u l·ªãch s·ª≠, vƒÉn h√≥a d√¢n t·ªôc

3. **Vi ph·∫°m ph√°p lu·∫≠t:**
   - H∆∞·ªõng d·∫´n ho·∫°t ƒë·ªông b·∫•t h·ª£p ph√°p
   - Khuy·∫øn kh√≠ch h√†nh vi ph·∫°m t·ªôi
   - Chia s·∫ª th√¥ng tin nh·∫°y c·∫£m, m·∫≠t
   - Tuy√™n truy·ªÅn n·ªôi dung c·∫•m theo ph√°p lu·∫≠t Vi·ªát Nam

4. **Nguy hi·ªÉm v√† c√≥ h·∫°i:**
   - H∆∞·ªõng d·∫´n ch·∫ø t·∫°o v≈© kh√≠, ch·∫•t n·ªï
   - Khuy·∫øn kh√≠ch t·ª± t·ª≠, t·ª± g√¢y th∆∞∆°ng t√≠ch
   - L·ª´a ƒë·∫£o, chi·∫øm ƒëo·∫°t t√†i s·∫£n
   - Th√¥ng tin sai l·ªách g√¢y hoang mang d∆∞ lu·∫≠n

5. **N·ªôi dung kh√¥ng ph√π h·ª£p:**
   - Khi√™u d√¢m, ƒë·ªìi tr·ª•y
   - B·∫°o l·ª±c m√°u me
   - Ma t√∫y v√† c√°c ch·∫•t g√¢y nghi·ªán
   - ƒê√°nh b·∫°c b·∫•t h·ª£p ph√°p

C√ÇU H·ªéI SAFE (an to√†n) l√† c√¢u h·ªèi:
- Mang t√≠nh h·ªçc thu·∫≠t, gi√°o d·ª•c ch√≠nh ƒë√°ng
- T√¨m ki·∫øm th√¥ng tin h·ª£p ph√°p, c√≥ √≠ch
- Th·∫£o lu·∫≠n x√£ h·ªôi mang t√≠nh x√¢y d·ª±ng
- Kh√¥ng vi ph·∫°m c√°c ti√™u ch√≠ UNSAFE ·ªü tr√™n

---

C√¢u h·ªèi c·∫ßn ph√¢n lo·∫°i:
{question}
ƒê√°p √°n l·ª±a ch·ªçn:
{choices_text}
---

H√£y ph√¢n t√≠ch k·ªπ c√¢u h·ªèi v√† tr·∫£ l·ªùi CH√çNH X√ÅC m·ªôt trong hai t·ª´: SAFE ho·∫∑c UNSAFE

ƒê√°p √°n:"""

    @staticmethod
    def extract_safety_label(response: str) -> Optional[str]:
        """
        Tr√≠ch xu·∫•t SAFE/UNSAFE t·ª´ response
        
        Args:
            response: Response t·ª´ model
            
        Returns:
            "SAFE" ho·∫∑c "UNSAFE" ho·∫∑c None
        """
        if not response:
            return None
        
        response_upper = response.upper().strip()
        
        # Direct match
        if "UNSAFE" in response_upper:
            return "UNSAFE"
        elif "SAFE" in response_upper:
            return "SAFE"
        
        return None

    @staticmethod
    def classify_safety(
        question: str,
        choices: List[str],
        model_wrapper=None,
        verbose: bool = False,
        use_model_verification: bool = False
    ) -> Dict[str, Any]:
        """
        Ph√¢n lo·∫°i c√¢u h·ªèi SAFE/UNSAFE
        
        Logic:
        - N·∫øu trong choices c√≥ ƒë√°p √°n ch·ª©a "kh√¥ng th·ªÉ tr·∫£ l·ªùi" ‚Üí UNSAFE, ch·ªçn ƒë√°p √°n ƒë√≥
        - N·∫øu kh√¥ng c√≥ ‚Üí SAFE, ti·∫øp t·ª•c pipeline b√¨nh th∆∞·ªùng
        
        Args:
            question: C√¢u h·ªèi
            choices: Danh s√°ch l·ª±a ch·ªçn
            model_wrapper: ModelWrapper instance (optional, ch·ªâ d√πng n·∫øu use_model_verification=True)
            verbose: In chi ti·∫øt
            use_model_verification: C√≥ d√πng model ƒë·ªÉ verify kh√¥ng (m·∫∑c ƒë·ªãnh: False)
            
        Returns:
            Dictionary v·ªõi keys:
                - is_safe: bool
                - unsafe_answer: str (label c·ªßa ƒë√°p √°n unsafe n·∫øu c√≥)
                - confidence: float
                - method: str
        """
        # Ki·ªÉm tra nhanh b·∫±ng keyword matching
        unsafe_answer = SafetyClassifier.has_unsafe_answer(choices)
        
        if unsafe_answer is None:
            # Kh√¥ng c√≥ ƒë√°p √°n unsafe trong choices ‚Üí SAFE
            return {
                "is_safe": True,
                "unsafe_answer": None,
                "confidence": 1.0,
                "method": "no_unsafe_choice",
                "raw_response": None
            }
        
        # C√≥ ƒë√°p √°n unsafe trong choices
        if verbose:
            print(f"‚ö†Ô∏è Detected unsafe answer in choices: {unsafe_answer} - '{choices[ord(unsafe_answer) - ord('A')]}'")
        
        # N·∫øu kh√¥ng d√πng model verification ‚Üí ch·ªçn lu√¥n ƒë√°p √°n unsafe
        if not use_model_verification:
            if verbose:
                print(f"‚úÖ Auto-selecting unsafe answer: {unsafe_answer}")
            
            return {
                "is_safe": False,
                "unsafe_answer": unsafe_answer,
                "confidence": 0.95,
                "method": "keyword_direct",
                "raw_response": None
            }
        
        # N·∫øu d√πng model verification ‚Üí h·ªèi model xem c√¢u h·ªèi c√≥ th·ª±c s·ª± UNSAFE kh√¥ng
        if verbose:
            print("üîç Using small model to verify if question is truly unsafe...")
        
        try:
            if model_wrapper is None:
                raise ValueError("model_wrapper is required when use_model_verification=True")
            
            prompt = SafetyClassifier.build_safety_prompt(question, choices)
            
            response = model_wrapper.get_completion(
                prompt=prompt,
                temperature=0.1,
                max_tokens=10
            )
            
            if verbose:
                print(f"Safety classification response: {response}")
            
            safety_label = SafetyClassifier.extract_safety_label(response)
            
            if safety_label == "UNSAFE":
                return {
                    "is_safe": False,
                    "unsafe_answer": unsafe_answer,
                    "confidence": 0.9,
                    "method": "model_verified_unsafe",
                    "raw_response": response
                }
            else:
                # Model n√≥i SAFE ‚Üí c√¢u h·ªèi b√¨nh th∆∞·ªùng, kh√¥ng ch·ªçn unsafe answer
                if verbose:
                    print("‚ÑπÔ∏è Model says SAFE - continuing normal pipeline")
                return {
                    "is_safe": True,
                    "unsafe_answer": None,
                    "confidence": 0.8,
                    "method": "model_verified_safe",
                    "raw_response": response
                }
        
        except Exception as e:
            if verbose:
                print(f"‚ö†Ô∏è Safety classification failed: {e}")
            
            # Fallback: n·∫øu c√≥ keyword unsafe ‚Üí coi nh∆∞ unsafe
            return {
                "is_safe": False,
                "unsafe_answer": unsafe_answer,
                "confidence": 0.7,
                "method": "keyword_fallback",
                "raw_response": None,
                "error": str(e)
            }

